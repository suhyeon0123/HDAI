{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import xmltodict\n",
    "import base64\n",
    "import numpy as np\n",
    "import array\n",
    "\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lead(path):\n",
    "    with open(path, 'rb') as xml:\n",
    "        ECG = xmltodict.parse(xml.read().decode('utf8'))\n",
    "    \n",
    "    augmentLeads = True\n",
    "    if path.split('/')[-1][0] == '5':\n",
    "        waveforms = ECG['RestingECG']['Waveform'][1]\n",
    "    elif path.split('/')[-1][0] == '6':\n",
    "        waveforms = ECG['RestingECG']['Waveform']\n",
    "        augmentLeads = False\n",
    "    else:\n",
    "        waveforms = ECG['RestingECG']['Waveform']\n",
    "    \n",
    "    leads = {}\n",
    "    \n",
    "    for lead in waveforms['LeadData']:\n",
    "        lead_data = lead['WaveFormData']\n",
    "        lead_b64  = base64.b64decode(lead_data)\n",
    "        lead_vals = np.array(array.array('h', lead_b64))\n",
    "        leads[ lead['LeadID'] ] = lead_vals\n",
    "    \n",
    "    if augmentLeads:\n",
    "        leads['III'] = np.subtract(leads['II'], leads['I'])\n",
    "        leads['aVR'] = np.add(leads['I'], leads['II'])*(-0.5)\n",
    "        leads['aVL'] = np.subtract(leads['I'], 0.5*leads['II'])\n",
    "        leads['aVF'] = np.subtract(leads['II'], 0.5*leads['I'])\n",
    "    \n",
    "    return leads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8_2_001879_ecg.xml\n",
      "6_2_003618_ecg.xml\n",
      "6_2_005055_ecg.xml\n",
      "6_2_003469_ecg.xml\n",
      "8_2_002164_ecg.xml\n",
      "8_2_007281_ecg.xml\n",
      "8_2_008783_ecg.xml\n",
      "8_2_007226_ecg.xml\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "error_train = ['6_2_003469_ecg.xml', '6_2_003618_ecg.xml', '6_2_005055_ecg.xml', '8_2_001879_ecg.xml', '8_2_002164_ecg.xml']\n",
    "error_valid = ['8_2_007281_ecg.xml', '8_2_008783_ecg.xml', '8_2_007226_ecg.xml']\n",
    "\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "valid_data = []\n",
    "valid_labels = []\n",
    "\n",
    "\n",
    "train_pathes = ['data/train/arrhythmia/', 'data/train/normal/']\n",
    "valid_pathes = ['data/validation/arrhythmia/', 'data/validation/normal/']\n",
    "\n",
    "error_decode = []   # 디코딩에 실패한 데이터들..\n",
    "# error_len = [] # 5000, 4999개를 맞추지 못한 데이터들.. 혹은 12개의 lead가 아닌것들..\n",
    "\n",
    "# train data\n",
    "for path in train_pathes:\n",
    "    for file in os.listdir(path):\n",
    "        \n",
    "        if file in error_train:\n",
    "            print(file)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            data = get_lead(path + file)\n",
    "        except Exception as e:\n",
    "            error_decode.append(path + file)\n",
    "        \n",
    "        listed_data = []\n",
    "        keys = sorted(data.keys())\n",
    "        for key in keys:\n",
    "            listed_data.append(data[key])\n",
    "        \n",
    "        flag = False\n",
    "        for idx, i in enumerate(listed_data):\n",
    "            if len(i) == 5000:\n",
    "                continue\n",
    "            elif len(i) == 4999:\n",
    "                listed_data[idx] = np.append(i, 0)\n",
    "            else:\n",
    "                flag = True\n",
    "        if flag:\n",
    "            continue\n",
    "        \n",
    "        train_data.append(listed_data)\n",
    "        if 'arrhythmia' in path:\n",
    "            train_labels.append(1)\n",
    "        else:\n",
    "            train_labels.append(0)\n",
    "            \n",
    "# valid data\n",
    "for path in valid_pathes:\n",
    "    for file in os.listdir(path):\n",
    "        \n",
    "        if file in error_valid:\n",
    "            print(file)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            data = get_lead(path + file)\n",
    "        except Exception as e:\n",
    "            error_decode.append(path + file)\n",
    "        \n",
    "        listed_data = []\n",
    "        keys = sorted(data.keys())\n",
    "        for key in keys:\n",
    "            listed_data.append(data[key])\n",
    "        \n",
    "        valid_data.append(listed_data)\n",
    "        if 'arrhythmia' in path:\n",
    "            valid_labels.append(1)\n",
    "        else:\n",
    "            valid_labels.append(0)\n",
    "\n",
    "print(len(error_decode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_lead_len = []\n",
    "for idx, i in enumerate(train_data):\n",
    "    if len(i) != 12:\n",
    "        error_lead_len.append(idx)\n",
    "for i in error_lead_len:\n",
    "    del train_data[i]\n",
    "    del train_labels[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 길이 및 lead 개수 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60912\n",
      "463928\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "# 데이터의 길이 분포 확인: valid는 모두 5000인것을 확인\n",
    "# train은 60912개가 4999, 36개가 1249, 1개가 4988\n",
    "# 위의 테스크는 먼저 4999개만 0의 패딩을 붙이고 나머지는 제외하는식으로 전처리함\n",
    "\n",
    "c4999 = 0\n",
    "c5000 = 0\n",
    "cx = 0\n",
    "\n",
    "for i in train_data:\n",
    "    for j in i:\n",
    "        if len(j) == 4999:\n",
    "            c4999 +=1\n",
    "        elif len(j) == 5000:\n",
    "            c5000 +=1\n",
    "        else:\n",
    "            cx +=1\n",
    "\n",
    "for i in valid_data:\n",
    "    for j in i:\n",
    "        if len(j) == 4999:\n",
    "            c4999 +=1\n",
    "        elif len(j) == 5000:\n",
    "            c5000 +=1\n",
    "        else:\n",
    "            cx +=1\n",
    "\n",
    "print(c4999)\n",
    "print(c5000)\n",
    "print(cx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43703\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 딱 한개의 9 lead의 데이터가 존재한다..\n",
    "v12 = 0\n",
    "v9 = 0\n",
    "vx = 0\n",
    "\n",
    "for i in train_data:\n",
    "    if len(i) == 12:\n",
    "        v12 +=1\n",
    "    elif len(i) == 9:\n",
    "        v9 += 1\n",
    "    else:\n",
    "        vx +=1\n",
    "        \n",
    "for i in valid_data:\n",
    "    if len(i) == 12:\n",
    "        v12 +=1\n",
    "    elif len(i) == 9:\n",
    "        v9 += 1\n",
    "    else:\n",
    "        vx +=1\n",
    "print(v12)\n",
    "print(v9)\n",
    "print(vx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15805/384750786.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272068694/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_data).float(), torch.tensor(train_labels))\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_data).float(), torch.tensor(train_labels))\n",
    "valid_dataset = torch.utils.data.TensorDataset(torch.tensor(valid_data).float(), torch.tensor(valid_labels))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual과 dropout 추가 필요\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, drop_out=0.0):\n",
    "        super(Classifier,self).__init__()\n",
    "        self.cnn1 = nn.Conv1d(in_channels=12, out_channels=32, kernel_size=5, padding=2) \n",
    "        self.cnn2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=5, padding=2)\n",
    "    \n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.pool2 = nn.MaxPool1d(5)\n",
    "        \n",
    "    \n",
    "        self.fc1 = nn.Linear(64 * 50, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.drop_out = nn.Dropout(p=drop_out)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.cnn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.cnn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu(self.cnn3(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(-1, 64*50)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.drop_out(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.drop_out(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        \n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x.view(-1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Classifier(drop_out=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "train \t\t loss mean 0.39449554681777954 accuracy: 0.9778859025846978\n",
      "validataion \t loss mean 0.456338495016098 accuracy: 0.9646017699115044 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoches = 100\n",
    "for i in range(epoches):\n",
    "    \n",
    "    # Train\n",
    "    loss_sum = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    model.train()\n",
    "    for e_num, (x,y) in enumerate(train_dataloader):\n",
    "        x, y = x.type(torch.FloatTensor).to(device), y.type(torch.FloatTensor).to(device)\n",
    "        model.zero_grad()\n",
    "        pred_y = model(x)\n",
    "        \n",
    "        loss=criterion(pred_y,y)\n",
    "        loss_sum+=loss.detach()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        true_labels.extend(y.cpu().numpy())\n",
    "        pred_labels.extend(np.around(pred_y.cpu().detach().numpy()))\n",
    "        \n",
    "        \n",
    "    acc=accuracy_score(true_labels,pred_labels)\n",
    "    print(f'epoch: {i}')\n",
    "    print(f'train \\t\\t loss mean {loss_sum/e_num} accuracy: {acc}')\n",
    "    \n",
    "    # Valid\n",
    "    loss_sum=0\n",
    "    true_labels=[]\n",
    "    pred_labels=[]\n",
    "    model.eval()\n",
    "    for e_num, (x,y) in enumerate(val_dataloader):\n",
    "        x, y = x.type(torch.FloatTensor).to(device), y.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        pred_y = model(x)\n",
    "        loss=criterion(pred_y,y)\n",
    "        \n",
    "        loss_sum+=loss.detach()\n",
    "        \n",
    "        true_labels.extend(y.cpu().numpy())\n",
    "        pred_labels.extend(np.around(pred_y.cpu().detach().numpy()))\n",
    "        \n",
    "        \n",
    "    acc=accuracy_score(true_labels,pred_labels)\n",
    "    print(f'validataion \\t loss mean {loss_sum/e_num} accuracy: {acc} ',end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDAI",
   "language": "python",
   "name": "hdai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
