{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import xmltodict\n",
    "import base64\n",
    "import numpy as np\n",
    "import array\n",
    "\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lead(path):\n",
    "    with open(path, 'rb') as xml:\n",
    "        ECG = xmltodict.parse(xml.read().decode('utf8'))\n",
    "    \n",
    "    augmentLeads = True\n",
    "    if path.split('/')[-1][0] == '5':\n",
    "        waveforms = ECG['RestingECG']['Waveform'][1]\n",
    "    elif path.split('/')[-1][0] == '6':\n",
    "        waveforms = ECG['RestingECG']['Waveform']\n",
    "        augmentLeads = False\n",
    "    else:\n",
    "        waveforms = ECG['RestingECG']['Waveform']\n",
    "    \n",
    "    leads = {}\n",
    "    \n",
    "    for lead in waveforms['LeadData']:\n",
    "        lead_data = lead['WaveFormData']\n",
    "        lead_b64  = base64.b64decode(lead_data)\n",
    "        lead_vals = np.array(array.array('h', lead_b64))\n",
    "        leads[ lead['LeadID'] ] = lead_vals\n",
    "    \n",
    "    if augmentLeads:\n",
    "        leads['III'] = np.subtract(leads['II'], leads['I'])\n",
    "        leads['aVR'] = np.add(leads['I'], leads['II'])*(-0.5)\n",
    "        leads['aVL'] = np.subtract(leads['I'], 0.5*leads['II'])\n",
    "        leads['aVF'] = np.subtract(leads['II'], 0.5*leads['I'])\n",
    "    \n",
    "    return leads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8_2_001879_ecg.xml\n",
      "6_2_003618_ecg.xml\n",
      "6_2_005055_ecg.xml\n",
      "6_2_003469_ecg.xml\n",
      "8_2_002164_ecg.xml\n",
      "8_2_007281_ecg.xml\n",
      "8_2_008783_ecg.xml\n",
      "8_2_007226_ecg.xml\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "error_train = ['6_2_003469_ecg.xml', '6_2_003618_ecg.xml', '6_2_005055_ecg.xml', '8_2_001879_ecg.xml', '8_2_002164_ecg.xml']\n",
    "error_valid = ['8_2_007281_ecg.xml', '8_2_008783_ecg.xml', '8_2_007226_ecg.xml']\n",
    "\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "valid_data = []\n",
    "valid_labels = []\n",
    "\n",
    "\n",
    "train_pathes = ['data/train/arrhythmia/', 'data/train/normal/']\n",
    "valid_pathes = ['data/validation/arrhythmia/', 'data/validation/normal/']\n",
    "\n",
    "error_decode = []   # 디코딩에 실패한 데이터들..\n",
    "# error_len = [] # 5000, 4999개를 맞추지 못한 데이터들.. 혹은 12개의 lead가 아닌것들..\n",
    "\n",
    "# train data\n",
    "for path in train_pathes:\n",
    "    for file in os.listdir(path):\n",
    "        \n",
    "        if file in error_train:\n",
    "            print(file)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            data = get_lead(path + file)\n",
    "        except Exception as e:\n",
    "            error_decode.append(path + file)\n",
    "        \n",
    "        listed_data = []\n",
    "        keys = sorted(data.keys())\n",
    "        for key in keys:\n",
    "            listed_data.append(data[key])\n",
    "        \n",
    "        flag = False\n",
    "        for idx, i in enumerate(listed_data):\n",
    "            if len(i) == 5000:\n",
    "                continue\n",
    "            elif len(i) == 4999:\n",
    "                listed_data[idx] = np.append(i, 0)\n",
    "            else:\n",
    "                flag = True\n",
    "        if flag:\n",
    "            continue\n",
    "        \n",
    "        train_data.append(listed_data)\n",
    "        if 'arrhythmia' in path:\n",
    "            train_labels.append(1)\n",
    "        else:\n",
    "            train_labels.append(0)\n",
    "            \n",
    "# valid data\n",
    "for path in valid_pathes:\n",
    "    for file in os.listdir(path):\n",
    "        \n",
    "        if file in error_valid:\n",
    "            print(file)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            data = get_lead(path + file)\n",
    "        except Exception as e:\n",
    "            error_decode.append(path + file)\n",
    "        \n",
    "        listed_data = []\n",
    "        keys = sorted(data.keys())\n",
    "        for key in keys:\n",
    "            listed_data.append(data[key])\n",
    "        \n",
    "        valid_data.append(listed_data)\n",
    "        if 'arrhythmia' in path:\n",
    "            valid_labels.append(1)\n",
    "        else:\n",
    "            valid_labels.append(0)\n",
    "\n",
    "print(len(error_decode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_lead_len = []\n",
    "for idx, i in enumerate(train_data):\n",
    "    if len(i) != 12:\n",
    "        error_lead_len.append(idx)\n",
    "for i in error_lead_len:\n",
    "    del train_data[i]\n",
    "    del train_labels[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 길이 및 lead 개수 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60912\n",
      "463928\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "# 데이터의 길이 분포 확인: valid는 모두 5000인것을 확인\n",
    "# train은 60912개가 4999, 36개가 1249, 1개가 4988\n",
    "# 위의 테스크는 먼저 4999개만 0의 패딩을 붙이고 나머지는 제외하는식으로 전처리함\n",
    "\n",
    "c4999 = 0\n",
    "c5000 = 0\n",
    "cx = 0\n",
    "\n",
    "for i in train_data:\n",
    "    for j in i:\n",
    "        if len(j) == 4999:\n",
    "            c4999 +=1\n",
    "        elif len(j) == 5000:\n",
    "            c5000 +=1\n",
    "        else:\n",
    "            cx +=1\n",
    "\n",
    "for i in valid_data:\n",
    "    for j in i:\n",
    "        if len(j) == 4999:\n",
    "            c4999 +=1\n",
    "        elif len(j) == 5000:\n",
    "            c5000 +=1\n",
    "        else:\n",
    "            cx +=1\n",
    "\n",
    "print(c4999)\n",
    "print(c5000)\n",
    "print(cx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43703\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 딱 한개의 9 lead의 데이터가 존재한다..\n",
    "v12 = 0\n",
    "v9 = 0\n",
    "vx = 0\n",
    "\n",
    "for i in train_data:\n",
    "    if len(i) == 12:\n",
    "        v12 +=1\n",
    "    elif len(i) == 9:\n",
    "        v9 += 1\n",
    "    else:\n",
    "        vx +=1\n",
    "        \n",
    "for i in valid_data:\n",
    "    if len(i) == 12:\n",
    "        v12 +=1\n",
    "    elif len(i) == 9:\n",
    "        v9 += 1\n",
    "    else:\n",
    "        vx +=1\n",
    "print(v12)\n",
    "print(v9)\n",
    "print(vx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15805/384750786.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272068694/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_data).float(), torch.tensor(train_labels))\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_data).float(), torch.tensor(train_labels))\n",
    "valid_dataset = torch.utils.data.TensorDataset(torch.tensor(valid_data).float(), torch.tensor(valid_labels))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual과 dropout 추가 필요\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier,self).__init__()\n",
    "        self.cnn1 = nn.Conv1d(in_channels=12, out_channels=32, kernel_size=5, padding=2) \n",
    "        self.cnn2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=5, padding=2)\n",
    "    \n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.pool2 = nn.MaxPool1d(5)\n",
    "        \n",
    "    \n",
    "        self.fc1 = nn.Linear(64 * 50, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.cnn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.cnn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu(self.cnn3(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(-1, 64*50)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        \n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x.view(-1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Classifier().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train \t\t loss mean 0.5464473366737366 accuracy: 0.8162908042426115\n",
      "validataion \t loss mean 0.46484997868537903 accuracy: 0.9117102284420663 \n",
      "\n",
      "train \t\t loss mean 0.4627467095851898 accuracy: 0.9137061064771909\n",
      "validataion \t loss mean 0.437894344329834 accuracy: 0.9415517596213212 \n",
      "\n",
      "train \t\t loss mean 0.4433371424674988 accuracy: 0.9329626197096077\n",
      "validataion \t loss mean 0.4295545518398285 accuracy: 0.9528709611031076 \n",
      "\n",
      "train \t\t loss mean 0.4339553415775299 accuracy: 0.9406858202038925\n",
      "validataion \t loss mean 0.4264028072357178 accuracy: 0.9454620292241201 \n",
      "\n",
      "train \t\t loss mean 0.4276195466518402 accuracy: 0.9459118525383585\n",
      "validataion \t loss mean 0.42061182856559753 accuracy: 0.9584276600123482 \n",
      "\n",
      "train \t\t loss mean 0.4218957722187042 accuracy: 0.95064874884152\n",
      "validataion \t loss mean 0.4220859408378601 accuracy: 0.956163819715991 \n",
      "\n",
      "train \t\t loss mean 0.4186888039112091 accuracy: 0.9529399649881578\n",
      "validataion \t loss mean 0.4181143045425415 accuracy: 0.9627495369417576 \n",
      "\n",
      "train \t\t loss mean 0.41482535004615784 accuracy: 0.9562866852023478\n",
      "validataion \t loss mean 0.41909316182136536 accuracy: 0.959250874665569 \n",
      "\n",
      "train \t\t loss mean 0.41226479411125183 accuracy: 0.9591442693852332\n",
      "validataion \t loss mean 0.41893383860588074 accuracy: 0.9615147149619263 \n",
      "\n",
      "train \t\t loss mean 0.4096607565879822 accuracy: 0.9610493255071568\n",
      "validataion \t loss mean 0.41726648807525635 accuracy: 0.9660423955546409 \n",
      "\n",
      "train \t\t loss mean 0.40804731845855713 accuracy: 0.963186077643909\n",
      "validataion \t loss mean 0.42045819759368896 accuracy: 0.9551348013994649 \n",
      "\n",
      "train \t\t loss mean 0.4067837595939636 accuracy: 0.9640356296982803\n",
      "validataion \t loss mean 0.4403240978717804 accuracy: 0.9590450710022639 \n",
      "\n",
      "train \t\t loss mean 0.40526026487350464 accuracy: 0.9656832458037278\n",
      "validataion \t loss mean 0.4439922273159027 accuracy: 0.9545173904095493 \n",
      "\n",
      "train \t\t loss mean 0.4033162295818329 accuracy: 0.967073421892699\n",
      "validataion \t loss mean 0.45575302839279175 accuracy: 0.9448446182342045 \n",
      "\n",
      "train \t\t loss mean 0.4020463824272156 accuracy: 0.9684635979816703\n",
      "validataion \t loss mean 0.43244388699531555 accuracy: 0.9676888248610825 \n",
      "\n",
      "train \t\t loss mean 0.4009959101676941 accuracy: 0.969956750077232\n",
      "validataion \t loss mean 0.4475913941860199 accuracy: 0.9543115867462441 \n",
      "\n",
      "train \t\t loss mean 0.40021005272865295 accuracy: 0.9701627020904129\n",
      "validataion \t loss mean 0.4329673945903778 accuracy: 0.9678946285243877 \n",
      "\n",
      "train \t\t loss mean 0.3991873264312744 accuracy: 0.9715786221810319\n",
      "validataion \t loss mean 0.44992873072624207 accuracy: 0.9528709611031076 \n",
      "\n",
      "train \t\t loss mean 0.39795127511024475 accuracy: 0.9727885902584698\n",
      "validataion \t loss mean 0.43513426184654236 accuracy: 0.9681004321876929 \n",
      "\n",
      "train \t\t loss mean 0.39729779958724976 accuracy: 0.973715374317784\n",
      "validataion \t loss mean 0.4572635293006897 accuracy: 0.949166495163614 \n",
      "\n",
      "train \t\t loss mean 0.39660143852233887 accuracy: 0.9740757903408506\n",
      "validataion \t loss mean 0.450838178396225 accuracy: 0.9567812307059066 \n",
      "\n",
      "train \t\t loss mean 0.395536333322525 accuracy: 0.9752342704149933\n",
      "validataion \t loss mean 0.4443863034248352 accuracy: 0.9623379296151472 \n",
      "\n",
      "train \t\t loss mean 0.39455506205558777 accuracy: 0.9760323344660694\n",
      "validataion \t loss mean 0.4555702209472656 accuracy: 0.9557522123893806 \n",
      "\n",
      "train \t\t loss mean 0.3947587311267853 accuracy: 0.9762382864792504\n",
      "validataion \t loss mean 0.45078277587890625 accuracy: 0.9567812307059066 \n",
      "\n",
      "train \t\t loss mean 0.3941352367401123 accuracy: 0.9772423025435074\n",
      "validataion \t loss mean 0.44418731331825256 accuracy: 0.9623379296151472 \n",
      "\n",
      "train \t\t loss mean 0.39378824830055237 accuracy: 0.9776284625682216\n",
      "validataion \t loss mean 0.4836636185646057 accuracy: 0.9362008643753859 \n",
      "\n",
      "train \t\t loss mean 0.39289045333862305 accuracy: 0.9781175985995263\n",
      "validataion \t loss mean 0.44208037853240967 accuracy: 0.963161144268368 \n",
      "\n",
      "train \t\t loss mean 0.3930528461933136 accuracy: 0.9782205746061168\n",
      "validataion \t loss mean 0.4444299340248108 accuracy: 0.9654249845647253 \n",
      "\n",
      "train \t\t loss mean 0.39178842306137085 accuracy: 0.9792245906703738\n",
      "validataion \t loss mean 0.4576244056224823 accuracy: 0.9541057830829389 \n",
      "\n",
      "train \t\t loss mean 0.3924413025379181 accuracy: 0.9788899186489548\n",
      "validataion \t loss mean 0.4311973750591278 accuracy: 0.9713932908005762 \n",
      "\n",
      "train \t\t loss mean 0.3917597532272339 accuracy: 0.9798681907115642\n",
      "validataion \t loss mean 0.43046218156814575 accuracy: 0.9715990944638815 \n",
      "\n",
      "train \t\t loss mean 0.3915153443813324 accuracy: 0.9798939347132118\n",
      "validataion \t loss mean 0.44550222158432007 accuracy: 0.9629553406050627 \n",
      "\n",
      "train \t\t loss mean 0.3910672962665558 accuracy: 0.9806662547626404\n",
      "validataion \t loss mean 0.49433428049087524 accuracy: 0.9378472936818275 \n",
      "\n",
      "train \t\t loss mean 0.3906087279319763 accuracy: 0.9811039027906497\n",
      "validataion \t loss mean 0.43769118189811707 accuracy: 0.9705700761473555 \n",
      "\n",
      "train \t\t loss mean 0.39023807644844055 accuracy: 0.9815672948203069\n",
      "validataion \t loss mean 0.45123887062072754 accuracy: 0.9619263222885367 \n",
      "\n",
      "train \t\t loss mean 0.3897912800312042 accuracy: 0.9817217588301925\n",
      "validataion \t loss mean 0.46991539001464844 accuracy: 0.9541057830829389 \n",
      "\n",
      "train \t\t loss mean 0.39011844992637634 accuracy: 0.9817217588301925\n",
      "validataion \t loss mean 0.4778471887111664 accuracy: 0.9497839061535295 \n",
      "\n",
      "train \t\t loss mean 0.3897162079811096 accuracy: 0.9822881268664401\n",
      "validataion \t loss mean 0.47363510727882385 accuracy: 0.9528709611031076 \n",
      "\n",
      "train \t\t loss mean 0.3892306089401245 accuracy: 0.9827772628977448\n",
      "validataion \t loss mean 0.45101264119148254 accuracy: 0.9646017699115044 \n",
      "\n",
      "train \t\t loss mean 0.3893340229988098 accuracy: 0.9830604469158686\n",
      "validataion \t loss mean 0.4798698127269745 accuracy: 0.9481374768470878 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoches = 50\n",
    "for i in range(epoches):\n",
    "    \n",
    "    # Train\n",
    "    loss_sum = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    model.train()\n",
    "    for e_num, (x,y) in enumerate(train_dataloader):\n",
    "        x, y = x.type(torch.FloatTensor).to(device), y.type(torch.FloatTensor).to(device)\n",
    "        model.zero_grad()\n",
    "        pred_y = model(x)\n",
    "        \n",
    "        loss=criterion(pred_y,y)\n",
    "        loss_sum+=loss.detach()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        true_labels.extend(y.cpu().numpy())\n",
    "        pred_labels.extend(np.around(pred_y.cpu().detach().numpy()))\n",
    "        \n",
    "        \n",
    "    acc=accuracy_score(true_labels,pred_labels)\n",
    "    print(f'train \\t\\t loss mean {loss_sum/e_num} accuracy: {acc}')\n",
    "    \n",
    "    # Valid\n",
    "    loss_sum=0\n",
    "    true_labels=[]\n",
    "    pred_labels=[]\n",
    "    model.eval()\n",
    "    for e_num, (x,y) in enumerate(val_dataloader):\n",
    "        x, y = x.type(torch.FloatTensor).to(device), y.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        pred_y = model(x)\n",
    "        loss=criterion(pred_y,y)\n",
    "        \n",
    "        loss_sum+=loss.detach()\n",
    "        \n",
    "        true_labels.extend(y.cpu().numpy())\n",
    "        pred_labels.extend(np.around(pred_y.cpu().detach().numpy()))\n",
    "        \n",
    "        \n",
    "    acc=accuracy_score(true_labels,pred_labels)\n",
    "    print(f'validataion \\t loss mean {loss_sum/e_num} accuracy: {acc} ',end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDAI",
   "language": "python",
   "name": "hdai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
