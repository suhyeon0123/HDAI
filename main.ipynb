{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import xmltodict\n",
    "import base64\n",
    "import numpy as np\n",
    "import array\n",
    "\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, roc_auc_score\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lead(path):\n",
    "    with open(path, 'rb') as xml:\n",
    "        ECG = xmltodict.parse(xml.read().decode('utf8'))\n",
    "    \n",
    "    augmentLeads = True\n",
    "    if path.split('/')[-1][0] == '5':\n",
    "        waveforms = ECG['RestingECG']['Waveform'][1]\n",
    "    elif path.split('/')[-1][0] == '6':\n",
    "        waveforms = ECG['RestingECG']['Waveform']\n",
    "        augmentLeads = False\n",
    "    else:\n",
    "        waveforms = ECG['RestingECG']['Waveform']\n",
    "    \n",
    "    leads = {}\n",
    "    \n",
    "    for lead in waveforms['LeadData']:\n",
    "        lead_data = lead['WaveFormData']\n",
    "        lead_b64  = base64.b64decode(lead_data)\n",
    "        lead_vals = np.array(array.array('h', lead_b64))\n",
    "        leads[ lead['LeadID'] ] = lead_vals\n",
    "    \n",
    "    if augmentLeads:\n",
    "        leads['III'] = np.subtract(leads['II'], leads['I'])\n",
    "        leads['aVR'] = np.add(leads['I'], leads['II'])*(-0.5)\n",
    "        leads['aVL'] = np.subtract(leads['I'], 0.5*leads['II'])\n",
    "        leads['aVF'] = np.subtract(leads['II'], 0.5*leads['I'])\n",
    "    \n",
    "    return leads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argument should be a bytes-like object or ASCII string, not 'NoneType'\n",
      "bytes length not a multiple of item size\n",
      "argument should be a bytes-like object or ASCII string, not 'NoneType'\n",
      "argument should be a bytes-like object or ASCII string, not 'NoneType'\n",
      "argument should be a bytes-like object or ASCII string, not 'NoneType'\n",
      "argument should be a bytes-like object or ASCII string, not 'NoneType'\n",
      "argument should be a bytes-like object or ASCII string, not 'NoneType'\n",
      "argument should be a bytes-like object or ASCII string, not 'NoneType'\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "error_files = ['6_2_003469_ecg.xml', '6_2_003618_ecg.xml', '6_2_005055_ecg.xml', '8_2_001879_ecg.xml', '8_2_002164_ecg.xml', '8_2_007281_ecg.xml', '8_2_008783_ecg.xml', '8_2_007226_ecg.xml']\n",
    "\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "valid_data = []\n",
    "valid_labels = []\n",
    "\n",
    "\n",
    "train_pathes = ['data/train/arrhythmia/', 'data/train/normal/']\n",
    "valid_pathes = ['data/validation/arrhythmia/', 'data/validation/normal/']\n",
    "\n",
    "error_decode = []   # 디코딩에 실패한 데이터들..\n",
    "\n",
    "for idx1, pathes in enumerate([train_pathes, valid_pathes]):\n",
    "    \n",
    "    for path in pathes:\n",
    "        for file in os.listdir(path):\n",
    "\n",
    "            if file in error_files or 'ipynb' in file:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                data = get_lead(path + file)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                error_decode.append(path + file)\n",
    "\n",
    "            listed_data = []\n",
    "            keys = sorted(data.keys())\n",
    "            for key in keys:\n",
    "                listed_data.append(data[key])\n",
    "\n",
    "            for idx2, i in enumerate(listed_data):\n",
    "                if len(i) != 5000:\n",
    "                    listed_data[idx2] = np.append(i, np.zeros(5000-len(i)))\n",
    "\n",
    "                    \n",
    "            # save each train, valid data\n",
    "            if idx1== 0: \n",
    "                train_data.append(listed_data)\n",
    "                if 'arrhythmia' in path:\n",
    "                    train_labels.append(1)\n",
    "                else:\n",
    "                    train_labels.append(0)\n",
    "            else:\n",
    "                valid_data.append(listed_data)\n",
    "                if 'arrhythmia' in path:\n",
    "                    valid_labels.append(1)\n",
    "                else:\n",
    "                    valid_labels.append(0)\n",
    "\n",
    "print(len(error_decode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_lead_len = []\n",
    "for idx, i in enumerate(train_data):\n",
    "    if len(i) != 12:\n",
    "        error_lead_len.append(idx)\n",
    "for i in error_lead_len:\n",
    "    del train_data[i]\n",
    "    del train_labels[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 길이 및 lead 개수 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 데이터의 길이 분포 확인: valid는 모두 5000인것을 확인\n",
    " \n",
    " train은 60912개가 4999, 36개가 1249, 1개가 4988\n",
    " \n",
    " 위의 테스크는 먼저 4999개만 0의 패딩을 붙이고 나머지는 제외하는식으로 전처리함\n",
    "딱 한개의 9 lead의 데이터가 존재한다.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26088/498780499.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272068694/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_data).float(), torch.tensor(train_labels))\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_data).float(), torch.tensor(train_labels))\n",
    "valid_dataset = torch.utils.data.TensorDataset(torch.tensor(valid_data).float(), torch.tensor(valid_labels))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual과 dropout 추가 필요\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, drop_out=0.0):\n",
    "        super(Classifier,self).__init__()\n",
    "        self.cnn1 = nn.Conv1d(in_channels=12, out_channels=32, kernel_size=5, padding=2) \n",
    "        self.cnn2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=5, padding=2)\n",
    "    \n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.pool2 = nn.MaxPool1d(5)\n",
    "        \n",
    "    \n",
    "        self.fc1 = nn.Linear(64 * 50, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.drop_out = nn.Dropout(p=drop_out)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.cnn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.cnn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu(self.cnn3(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(-1, 64*50)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.drop_out(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.drop_out(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        \n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x.view(-1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "drop_out = 0.2\n",
    "l2_norm = 1e-6\n",
    "lr = 0.00001\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Classifier(drop_out=drop_out).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_norm)\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "train \t\t loss mean 0.4976399838924408 accuracy: 0.8779835390946502\n",
      "NEW RECODE!\n",
      "validataion \t loss mean  0.4653, auc_score :  0.9330, accuracy:  0.9306 \n",
      "\n",
      "epoch: 1\n",
      "train \t\t loss mean 0.4718281924724579 accuracy: 0.9058384773662551\n",
      "NEW RECODE!\n",
      "validataion \t loss mean  0.4476, auc_score :  0.9343, accuracy:  0.9304 \n",
      "\n",
      "epoch: 2\n",
      "train \t\t loss mean 0.457766592502594 accuracy: 0.9171296296296296\n",
      "NEW RECODE!\n",
      "validataion \t loss mean  0.4382, auc_score :  0.9458, accuracy:  0.9428 \n",
      "\n",
      "epoch: 3\n",
      "train \t\t loss mean 0.44914573431015015 accuracy: 0.9257973251028807\n",
      "NEW RECODE!\n",
      "validataion \t loss mean  0.4318, auc_score :  0.9519, accuracy:  0.9498 \n",
      "\n",
      "epoch: 4\n",
      "train \t\t loss mean 0.44436272978782654 accuracy: 0.9317901234567901\n",
      "NEW RECODE!\n",
      "validataion \t loss mean  0.4275, auc_score :  0.9582, accuracy:  0.9568 \n",
      "\n",
      "epoch: 5\n",
      "train \t\t loss mean 0.4361080527305603 accuracy: 0.9357253086419753\n",
      "validataion \t loss mean  0.4285, auc_score :  0.9559, accuracy:  0.9545 \n",
      "\n",
      "epoch: 6\n",
      "train \t\t loss mean 0.43264976143836975 accuracy: 0.9392232510288065\n",
      "NEW RECODE!\n",
      "validataion \t loss mean  0.4238, auc_score :  0.9656, accuracy:  0.9660 \n",
      "\n",
      "epoch: 7\n",
      "train \t\t loss mean 0.4318772554397583 accuracy: 0.9407664609053498\n",
      "validataion \t loss mean  0.4270, auc_score :  0.9568, accuracy:  0.9549 \n",
      "\n",
      "epoch: 8\n",
      "train \t\t loss mean 0.4265097975730896 accuracy: 0.9457561728395062\n",
      "validataion \t loss mean  0.4239, auc_score :  0.9627, accuracy:  0.9619 \n",
      "\n",
      "epoch: 9\n",
      "train \t\t loss mean 0.4235000014305115 accuracy: 0.9473508230452675\n",
      "validataion \t loss mean  0.4377, auc_score :  0.9498, accuracy:  0.9469 \n",
      "\n",
      "epoch: 10\n",
      "train \t\t loss mean 0.42308658361434937 accuracy: 0.9500514403292181\n",
      "NEW RECODE!\n",
      "validataion \t loss mean  0.4220, auc_score :  0.9662, accuracy:  0.9669 \n",
      "\n",
      "epoch: 11\n",
      "train \t\t loss mean 0.4188513159751892 accuracy: 0.9525462962962963\n",
      "NEW RECODE!\n",
      "validataion \t loss mean  0.4205, auc_score :  0.9692, accuracy:  0.9693 \n",
      "\n",
      "epoch: 12\n",
      "train \t\t loss mean 0.4172075688838959 accuracy: 0.9535493827160494\n",
      "validataion \t loss mean  0.4194, auc_score :  0.9674, accuracy:  0.9671 \n",
      "\n",
      "epoch: 13\n",
      "train \t\t loss mean 0.41589638590812683 accuracy: 0.9545781893004115\n",
      "validataion \t loss mean  0.4190, auc_score :  0.9687, accuracy:  0.9687 \n",
      "\n",
      "epoch: 14\n",
      "train \t\t loss mean 0.4137412905693054 accuracy: 0.9564814814814815\n",
      "validataion \t loss mean  0.4333, auc_score :  0.9564, accuracy:  0.9541 \n",
      "\n",
      "epoch: 15\n",
      "train \t\t loss mean 0.412054181098938 accuracy: 0.9581018518518518\n",
      "validataion \t loss mean  0.4210, auc_score :  0.9669, accuracy:  0.9658 \n",
      "\n",
      "epoch: 16\n",
      "train \t\t loss mean 0.41180887818336487 accuracy: 0.9588734567901235\n",
      "validataion \t loss mean  0.4354, auc_score :  0.9562, accuracy:  0.9541 \n",
      "\n",
      "epoch: 17\n",
      "train \t\t loss mean 0.41025853157043457 accuracy: 0.9594907407407407\n",
      "validataion \t loss mean  0.4221, auc_score :  0.9683, accuracy:  0.9675 \n",
      "\n",
      "epoch: 18\n",
      "train \t\t loss mean 0.4081103205680847 accuracy: 0.961548353909465\n",
      "validataion \t loss mean  0.4316, auc_score :  0.9637, accuracy:  0.9621 \n",
      "\n",
      "epoch: 19\n",
      "train \t\t loss mean 0.4078322649002075 accuracy: 0.9622685185185185\n",
      "validataion \t loss mean  0.4435, auc_score :  0.9670, accuracy:  0.9662 \n",
      "\n",
      "epoch: 20\n",
      "train \t\t loss mean 0.40680116415023804 accuracy: 0.9628086419753087\n",
      "validataion \t loss mean  0.4441, auc_score :  0.9685, accuracy:  0.9679 \n",
      "\n",
      "epoch: 21\n",
      "train \t\t loss mean 0.4046897292137146 accuracy: 0.9648662551440329\n",
      "validataion \t loss mean  0.4674, auc_score :  0.9649, accuracy:  0.9634 \n",
      "\n",
      "epoch: 22\n",
      "train \t\t loss mean 0.4038922190666199 accuracy: 0.9665895061728395\n",
      "validataion \t loss mean  0.4471, auc_score :  0.9682, accuracy:  0.9675 \n",
      "\n",
      "epoch: 23\n",
      "train \t\t loss mean 0.40373265743255615 accuracy: 0.9664094650205761\n",
      "validataion \t loss mean  0.4694, auc_score :  0.9637, accuracy:  0.9621 \n",
      "\n",
      "epoch: 24\n",
      "train \t\t loss mean 0.40261393785476685 accuracy: 0.9673611111111111\n",
      "validataion \t loss mean  0.4884, auc_score :  0.9635, accuracy:  0.9617 \n",
      "\n",
      "epoch: 25\n",
      "train \t\t loss mean 0.40169912576675415 accuracy: 0.9678497942386831\n",
      "validataion \t loss mean  0.4831, auc_score :  0.9659, accuracy:  0.9644 \n",
      "\n",
      "epoch: 26\n",
      "train \t\t loss mean 0.40162622928619385 accuracy: 0.968724279835391\n",
      "validataion \t loss mean  0.4884, auc_score :  0.9605, accuracy:  0.9590 \n",
      "\n",
      "epoch: 27\n",
      "train \t\t loss mean 0.40035754442214966 accuracy: 0.9698045267489712\n",
      "NEW RECODE!\n",
      "validataion \t loss mean  0.4391, auc_score :  0.9729, accuracy:  0.9728 \n",
      "\n",
      "epoch: 28\n",
      "train \t\t loss mean 0.399362176656723 accuracy: 0.9710390946502058\n",
      "validataion \t loss mean  0.4892, auc_score :  0.9644, accuracy:  0.9627 \n",
      "\n",
      "epoch: 29\n",
      "train \t\t loss mean 0.39913904666900635 accuracy: 0.971141975308642\n",
      "validataion \t loss mean  0.4953, auc_score :  0.9604, accuracy:  0.9582 \n",
      "\n",
      "epoch: 30\n",
      "train \t\t loss mean 0.3985188901424408 accuracy: 0.9719135802469135\n",
      "validataion \t loss mean  0.4848, auc_score :  0.9685, accuracy:  0.9671 \n",
      "\n",
      "epoch: 31\n",
      "train \t\t loss mean 0.3982815742492676 accuracy: 0.9721707818930041\n",
      "validataion \t loss mean  0.4973, auc_score :  0.9594, accuracy:  0.9574 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoches = 200\n",
    "\n",
    "best_auc = 0\n",
    "best_epoch = -1\n",
    "best_pred = []\n",
    "\n",
    "for i in range(epoches):\n",
    "    \n",
    "    # Train\n",
    "    loss_sum = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    model.train()\n",
    "    for e_num, (x,y) in enumerate(train_dataloader):\n",
    "        x, y = x.type(torch.FloatTensor).to(device), y.type(torch.FloatTensor).to(device)\n",
    "        model.zero_grad()\n",
    "        pred_y = model(x)\n",
    "        \n",
    "        loss=criterion(pred_y,y)\n",
    "        loss_sum+=loss.detach()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        true_labels.extend(y.cpu().numpy())\n",
    "        pred_labels.extend(np.around(pred_y.cpu().detach().numpy()))\n",
    "        \n",
    "        \n",
    "    acc=accuracy_score(true_labels,pred_labels)\n",
    "    print(f'epoch: {i}')\n",
    "    print(f'train \\t\\t loss mean {loss_sum/e_num : .4f} accuracy: {acc : .4f}')\n",
    "    \n",
    "    # Valid\n",
    "    loss_sum=0\n",
    "    true_labels=[]\n",
    "    pred_labels=[]\n",
    "    model.eval()\n",
    "    for e_num, (x,y) in enumerate(val_dataloader):\n",
    "        x, y = x.type(torch.FloatTensor).to(device), y.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        pred_y = model(x)\n",
    "        loss=criterion(pred_y,y)\n",
    "        \n",
    "        loss_sum+=loss.detach()\n",
    "        \n",
    "        true_labels.extend(y.cpu().numpy())\n",
    "        pred_labels.extend(np.around(pred_y.cpu().detach().numpy()))\n",
    "        \n",
    "        \n",
    "    acc = accuracy_score(true_labels,pred_labels)\n",
    "    auc = roc_auc_score(true_labels,pred_labels)\n",
    "    \n",
    "    print(f'validataion \\t loss mean {loss_sum/e_num : .4f}, auc_score : {auc : .4f}, accuracy: {acc : .4f} ', end='\\n\\n')\n",
    "    if auc > best_auc:\n",
    "        print(\"NEW RECODE!\")\n",
    "        best_pred = pred_labels\n",
    "        best_auc = auc\n",
    "        best_epoch = i\n",
    "        torch.save(model.state_dict(), f'cnn_model_{best_auc : .4f}.h5')\n",
    "                   \n",
    "    \n",
    "        \n",
    "\n",
    "print(f'best validation acc = {best_auc : .4f}, in epoch {best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fper, tper):\n",
    "    plt.plot(fper, tper, color='red', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fper, tper, thresholds = roc_curve(true_labels,best_acc_pred)\n",
    "plot_roc_curve(fper, tper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDAI",
   "language": "python",
   "name": "hdai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
