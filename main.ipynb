{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import xmltodict\n",
    "import base64\n",
    "import numpy as np\n",
    "import array\n",
    "\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, roc_auc_score\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lead(path):\n",
    "    with open(path, 'rb') as xml:\n",
    "        ECG = xmltodict.parse(xml.read().decode('utf8'))\n",
    "    \n",
    "    augmentLeads = True\n",
    "    if path.split('/')[-1][0] == '5':\n",
    "        waveforms = ECG['RestingECG']['Waveform'][1]\n",
    "    elif path.split('/')[-1][0] == '6':\n",
    "        waveforms = ECG['RestingECG']['Waveform']\n",
    "        augmentLeads = False\n",
    "    else:\n",
    "        waveforms = ECG['RestingECG']['Waveform']\n",
    "    \n",
    "    leads = {}\n",
    "    \n",
    "    for lead in waveforms['LeadData']:\n",
    "        lead_data = lead['WaveFormData']\n",
    "        lead_b64  = base64.b64decode(lead_data)\n",
    "        lead_vals = np.array(array.array('h', lead_b64))\n",
    "        leads[ lead['LeadID'] ] = lead_vals\n",
    "    \n",
    "    if augmentLeads:\n",
    "        leads['III'] = np.subtract(leads['II'], leads['I'])\n",
    "        leads['aVR'] = np.add(leads['I'], leads['II'])*(-0.5)\n",
    "        leads['aVL'] = np.subtract(leads['I'], 0.5*leads['II'])\n",
    "        leads['aVF'] = np.subtract(leads['II'], 0.5*leads['I'])\n",
    "    \n",
    "    return leads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argument should be a bytes-like object or ASCII string, not 'NoneType'\n",
      "bytes length not a multiple of item size\n",
      "argument should be a bytes-like object or ASCII string, not 'NoneType'\n",
      "argument should be a bytes-like object or ASCII string, not 'NoneType'\n",
      "argument should be a bytes-like object or ASCII string, not 'NoneType'\n",
      "argument should be a bytes-like object or ASCII string, not 'NoneType'\n",
      "argument should be a bytes-like object or ASCII string, not 'NoneType'\n",
      "argument should be a bytes-like object or ASCII string, not 'NoneType'\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "error_files = ['6_2_003469_ecg.xml', '6_2_003618_ecg.xml', '6_2_005055_ecg.xml', '8_2_001879_ecg.xml', '8_2_002164_ecg.xml', '8_2_007281_ecg.xml', '8_2_008783_ecg.xml', '8_2_007226_ecg.xml']\n",
    "\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "valid_data = []\n",
    "valid_labels = []\n",
    "\n",
    "\n",
    "train_pathes = ['data/train/arrhythmia/', 'data/train/normal/']\n",
    "valid_pathes = ['data/validation/arrhythmia/', 'data/validation/normal/']\n",
    "\n",
    "error_decode = []   # 디코딩에 실패한 데이터들..\n",
    "\n",
    "for idx1, pathes in enumerate([train_pathes, valid_pathes]):\n",
    "    \n",
    "    for path in pathes:\n",
    "        for file in os.listdir(path):\n",
    "\n",
    "            if file in error_files or 'ipynb' in file:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                data = get_lead(path + file)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                error_decode.append(path + file)\n",
    "\n",
    "            listed_data = []\n",
    "            keys = sorted(data.keys())\n",
    "            for key in keys:\n",
    "                listed_data.append(data[key])\n",
    "\n",
    "            for idx2, i in enumerate(listed_data):\n",
    "                if len(i) != 5000:\n",
    "                    listed_data[idx2] = np.append(i, np.zeros(5000-len(i)))\n",
    "\n",
    "                    \n",
    "            # save each train, valid data\n",
    "            if idx1== 0: \n",
    "                train_data.append(listed_data)\n",
    "                if 'arrhythmia' in path:\n",
    "                    train_labels.append(1)\n",
    "                else:\n",
    "                    train_labels.append(0)\n",
    "            else:\n",
    "                valid_data.append(listed_data)\n",
    "                if 'arrhythmia' in path:\n",
    "                    valid_labels.append(1)\n",
    "                else:\n",
    "                    valid_labels.append(0)\n",
    "\n",
    "print(len(error_decode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_lead_len = []\n",
    "for idx, i in enumerate(train_data):\n",
    "    if len(i) != 12:\n",
    "        error_lead_len.append(idx)\n",
    "for i in error_lead_len:\n",
    "    del train_data[i]\n",
    "    del train_labels[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 길이 및 lead 개수 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 데이터의 길이 분포 확인: valid는 모두 5000인것을 확인\n",
    " \n",
    " train은 60912개가 4999, 36개가 1249, 1개가 4988\n",
    " \n",
    " 위의 테스크는 먼저 4999개만 0의 패딩을 붙이고 나머지는 제외하는식으로 전처리함\n",
    "딱 한개의 9 lead의 데이터가 존재한다.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26088/498780499.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272068694/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_data).float(), torch.tensor(train_labels))\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_data).float(), torch.tensor(train_labels))\n",
    "valid_dataset = torch.utils.data.TensorDataset(torch.tensor(valid_data).float(), torch.tensor(valid_labels))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual과 dropout 추가 필요\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, drop_out=0.0):\n",
    "        super(Classifier,self).__init__()\n",
    "        self.cnn1 = nn.Conv1d(in_channels=12, out_channels=32, kernel_size=5, padding=2) \n",
    "        self.cnn2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.cnn3 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=5, padding=2)\n",
    "    \n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.pool2 = nn.MaxPool1d(5)\n",
    "        \n",
    "    \n",
    "        self.fc1 = nn.Linear(64 * 50, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.drop_out = nn.Dropout(p=drop_out)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.cnn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.cnn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu(self.cnn3(x))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(-1, 64*50)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.drop_out(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.drop_out(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        \n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x.view(-1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "drop_out = 0.2\n",
    "l2_norm = 1e-6\n",
    "lr = 0.00001\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = Classifier(drop_out=drop_out).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_norm)\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "train \t\t loss mean  0.6294 accuracy:  0.6893\n",
      "validataion \t loss mean  0.5309, auc_score :  0.8793, accuracy:  0.8765 \n",
      "\n",
      "NEW RECODE!\n",
      "epoch: 1\n",
      "train \t\t loss mean  0.5070 accuracy:  0.8688\n",
      "validataion \t loss mean  0.4618, auc_score :  0.9386, accuracy:  0.9370 \n",
      "\n",
      "NEW RECODE!\n",
      "epoch: 2\n",
      "train \t\t loss mean  0.4730 accuracy:  0.9028\n",
      "validataion \t loss mean  0.4434, auc_score :  0.9474, accuracy:  0.9459 \n",
      "\n",
      "NEW RECODE!\n",
      "epoch: 3\n",
      "train \t\t loss mean  0.4580 accuracy:  0.9163\n",
      "validataion \t loss mean  0.4404, auc_score :  0.9386, accuracy:  0.9350 \n",
      "\n",
      "epoch: 4\n",
      "train \t\t loss mean  0.4488 accuracy:  0.9245\n",
      "validataion \t loss mean  0.4302, auc_score :  0.9578, accuracy:  0.9570 \n",
      "\n",
      "NEW RECODE!\n",
      "epoch: 5\n",
      "train \t\t loss mean  0.4415 accuracy:  0.9315\n",
      "validataion \t loss mean  0.4251, auc_score :  0.9603, accuracy:  0.9597 \n",
      "\n",
      "NEW RECODE!\n",
      "epoch: 6\n",
      "train \t\t loss mean  0.4373 accuracy:  0.9349\n",
      "validataion \t loss mean  0.4258, auc_score :  0.9614, accuracy:  0.9609 \n",
      "\n",
      "NEW RECODE!\n",
      "epoch: 7\n",
      "train \t\t loss mean  0.4332 accuracy:  0.9393\n",
      "validataion \t loss mean  0.4221, auc_score :  0.9651, accuracy:  0.9648 \n",
      "\n",
      "NEW RECODE!\n",
      "epoch: 8\n",
      "train \t\t loss mean  0.4289 accuracy:  0.9421\n",
      "validataion \t loss mean  0.4240, auc_score :  0.9628, accuracy:  0.9630 \n",
      "\n",
      "epoch: 9\n",
      "train \t\t loss mean  0.4269 accuracy:  0.9444\n",
      "validataion \t loss mean  0.4423, auc_score :  0.9626, accuracy:  0.9617 \n",
      "\n",
      "epoch: 10\n",
      "train \t\t loss mean  0.4240 accuracy:  0.9471\n",
      "validataion \t loss mean  0.4234, auc_score :  0.9629, accuracy:  0.9625 \n",
      "\n",
      "epoch: 11\n",
      "train \t\t loss mean  0.4213 accuracy:  0.9487\n",
      "validataion \t loss mean  0.4547, auc_score :  0.9551, accuracy:  0.9529 \n",
      "\n",
      "epoch: 12\n",
      "train \t\t loss mean  0.4193 accuracy:  0.9508\n",
      "validataion \t loss mean  0.4391, auc_score :  0.9653, accuracy:  0.9652 \n",
      "\n",
      "NEW RECODE!\n",
      "epoch: 13\n",
      "train \t\t loss mean  0.4179 accuracy:  0.9529\n",
      "validataion \t loss mean  0.4398, auc_score :  0.9639, accuracy:  0.9642 \n",
      "\n",
      "epoch: 14\n",
      "train \t\t loss mean  0.4157 accuracy:  0.9543\n",
      "validataion \t loss mean  0.4554, auc_score :  0.9559, accuracy:  0.9537 \n",
      "\n",
      "epoch: 15\n",
      "train \t\t loss mean  0.4144 accuracy:  0.9564\n",
      "validataion \t loss mean  0.4383, auc_score :  0.9689, accuracy:  0.9687 \n",
      "\n",
      "NEW RECODE!\n",
      "epoch: 16\n",
      "train \t\t loss mean  0.4130 accuracy:  0.9564\n",
      "validataion \t loss mean  0.4541, auc_score :  0.9597, accuracy:  0.9580 \n",
      "\n",
      "epoch: 17\n",
      "train \t\t loss mean  0.4116 accuracy:  0.9582\n",
      "validataion \t loss mean  0.4351, auc_score :  0.9699, accuracy:  0.9700 \n",
      "\n",
      "NEW RECODE!\n",
      "epoch: 18\n",
      "train \t\t loss mean  0.4099 accuracy:  0.9602\n",
      "validataion \t loss mean  0.4391, auc_score :  0.9665, accuracy:  0.9662 \n",
      "\n",
      "epoch: 19\n",
      "train \t\t loss mean  0.4095 accuracy:  0.9608\n",
      "validataion \t loss mean  0.4491, auc_score :  0.9609, accuracy:  0.9595 \n",
      "\n",
      "epoch: 20\n",
      "train \t\t loss mean  0.4082 accuracy:  0.9616\n",
      "validataion \t loss mean  0.4416, auc_score :  0.9648, accuracy:  0.9644 \n",
      "\n",
      "epoch: 21\n",
      "train \t\t loss mean  0.4073 accuracy:  0.9637\n",
      "validataion \t loss mean  0.4731, auc_score :  0.9582, accuracy:  0.9562 \n",
      "\n",
      "epoch: 22\n",
      "train \t\t loss mean  0.4062 accuracy:  0.9640\n",
      "validataion \t loss mean  0.4478, auc_score :  0.9654, accuracy:  0.9638 \n",
      "\n",
      "epoch: 23\n",
      "train \t\t loss mean  0.4059 accuracy:  0.9641\n",
      "validataion \t loss mean  0.4534, auc_score :  0.9605, accuracy:  0.9586 \n",
      "\n",
      "epoch: 24\n",
      "train \t\t loss mean  0.4051 accuracy:  0.9651\n",
      "validataion \t loss mean  0.4480, auc_score :  0.9647, accuracy:  0.9638 \n",
      "\n",
      "epoch: 25\n",
      "train \t\t loss mean  0.4040 accuracy:  0.9663\n",
      "validataion \t loss mean  0.4665, auc_score :  0.9660, accuracy:  0.9650 \n",
      "\n",
      "epoch: 26\n",
      "train \t\t loss mean  0.4031 accuracy:  0.9668\n",
      "validataion \t loss mean  0.4445, auc_score :  0.9670, accuracy:  0.9665 \n",
      "\n",
      "epoch: 27\n",
      "train \t\t loss mean  0.4026 accuracy:  0.9687\n",
      "validataion \t loss mean  0.4727, auc_score :  0.9619, accuracy:  0.9605 \n",
      "\n",
      "epoch: 28\n",
      "train \t\t loss mean  0.4018 accuracy:  0.9690\n",
      "validataion \t loss mean  0.4619, auc_score :  0.9662, accuracy:  0.9652 \n",
      "\n",
      "epoch: 29\n",
      "train \t\t loss mean  0.4008 accuracy:  0.9698\n",
      "validataion \t loss mean  0.4613, auc_score :  0.9717, accuracy:  0.9712 \n",
      "\n",
      "NEW RECODE!\n",
      "epoch: 30\n",
      "train \t\t loss mean  0.4006 accuracy:  0.9704\n",
      "validataion \t loss mean  0.4599, auc_score :  0.9603, accuracy:  0.9586 \n",
      "\n",
      "epoch: 31\n",
      "train \t\t loss mean  0.3999 accuracy:  0.9704\n",
      "validataion \t loss mean  0.4724, auc_score :  0.9656, accuracy:  0.9642 \n",
      "\n",
      "epoch: 32\n",
      "train \t\t loss mean  0.3987 accuracy:  0.9717\n",
      "validataion \t loss mean  0.4502, auc_score :  0.9668, accuracy:  0.9652 \n",
      "\n",
      "epoch: 33\n",
      "train \t\t loss mean  0.3982 accuracy:  0.9719\n",
      "validataion \t loss mean  0.4397, auc_score :  0.9727, accuracy:  0.9722 \n",
      "\n",
      "NEW RECODE!\n",
      "epoch: 34\n",
      "train \t\t loss mean  0.3982 accuracy:  0.9721\n",
      "validataion \t loss mean  0.4411, auc_score :  0.9707, accuracy:  0.9706 \n",
      "\n",
      "epoch: 35\n",
      "train \t\t loss mean  0.3975 accuracy:  0.9736\n",
      "validataion \t loss mean  0.4896, auc_score :  0.9589, accuracy:  0.9566 \n",
      "\n",
      "epoch: 36\n",
      "train \t\t loss mean  0.3975 accuracy:  0.9738\n",
      "validataion \t loss mean  0.4624, auc_score :  0.9694, accuracy:  0.9685 \n",
      "\n",
      "epoch: 37\n",
      "train \t\t loss mean  0.3968 accuracy:  0.9737\n",
      "validataion \t loss mean  0.4725, auc_score :  0.9657, accuracy:  0.9644 \n",
      "\n",
      "epoch: 38\n",
      "train \t\t loss mean  0.3968 accuracy:  0.9753\n",
      "validataion \t loss mean  0.4669, auc_score :  0.9683, accuracy:  0.9677 \n",
      "\n",
      "epoch: 39\n",
      "train \t\t loss mean  0.3963 accuracy:  0.9745\n",
      "validataion \t loss mean  0.4775, auc_score :  0.9664, accuracy:  0.9650 \n",
      "\n",
      "epoch: 40\n",
      "train \t\t loss mean  0.3958 accuracy:  0.9759\n",
      "validataion \t loss mean  0.4730, auc_score :  0.9655, accuracy:  0.9648 \n",
      "\n",
      "epoch: 41\n",
      "train \t\t loss mean  0.3943 accuracy:  0.9768\n",
      "validataion \t loss mean  0.5005, auc_score :  0.9643, accuracy:  0.9627 \n",
      "\n",
      "epoch: 42\n",
      "train \t\t loss mean  0.3940 accuracy:  0.9772\n",
      "validataion \t loss mean  0.4768, auc_score :  0.9646, accuracy:  0.9634 \n",
      "\n",
      "epoch: 43\n",
      "train \t\t loss mean  0.3946 accuracy:  0.9767\n",
      "validataion \t loss mean  0.4805, auc_score :  0.9663, accuracy:  0.9648 \n",
      "\n",
      "epoch: 44\n",
      "train \t\t loss mean  0.3935 accuracy:  0.9778\n",
      "validataion \t loss mean  0.4688, auc_score :  0.9686, accuracy:  0.9677 \n",
      "\n",
      "epoch: 45\n",
      "train \t\t loss mean  0.3927 accuracy:  0.9789\n",
      "validataion \t loss mean  0.4690, auc_score :  0.9688, accuracy:  0.9683 \n",
      "\n",
      "epoch: 46\n",
      "train \t\t loss mean  0.3930 accuracy:  0.9786\n",
      "validataion \t loss mean  0.5131, auc_score :  0.9584, accuracy:  0.9560 \n",
      "\n",
      "epoch: 47\n",
      "train \t\t loss mean  0.3926 accuracy:  0.9785\n",
      "validataion \t loss mean  0.4988, auc_score :  0.9668, accuracy:  0.9656 \n",
      "\n",
      "epoch: 48\n",
      "train \t\t loss mean  0.3924 accuracy:  0.9797\n",
      "validataion \t loss mean  0.4738, auc_score :  0.9696, accuracy:  0.9685 \n",
      "\n",
      "epoch: 49\n",
      "train \t\t loss mean  0.3925 accuracy:  0.9802\n",
      "validataion \t loss mean  0.4393, auc_score :  0.9741, accuracy:  0.9743 \n",
      "\n",
      "NEW RECODE!\n",
      "epoch: 50\n",
      "train \t\t loss mean  0.3924 accuracy:  0.9797\n",
      "validataion \t loss mean  0.5261, auc_score :  0.9631, accuracy:  0.9613 \n",
      "\n",
      "epoch: 51\n",
      "train \t\t loss mean  0.3914 accuracy:  0.9805\n",
      "validataion \t loss mean  0.4970, auc_score :  0.9684, accuracy:  0.9673 \n",
      "\n",
      "epoch: 52\n",
      "train \t\t loss mean  0.3912 accuracy:  0.9808\n",
      "validataion \t loss mean  0.4688, auc_score :  0.9734, accuracy:  0.9728 \n",
      "\n",
      "epoch: 53\n",
      "train \t\t loss mean  0.3911 accuracy:  0.9809\n",
      "validataion \t loss mean  0.4784, auc_score :  0.9682, accuracy:  0.9673 \n",
      "\n",
      "epoch: 54\n",
      "train \t\t loss mean  0.3909 accuracy:  0.9807\n",
      "validataion \t loss mean  0.5203, auc_score :  0.9648, accuracy:  0.9632 \n",
      "\n",
      "epoch: 55\n",
      "train \t\t loss mean  0.3903 accuracy:  0.9818\n",
      "validataion \t loss mean  0.4579, auc_score :  0.9752, accuracy:  0.9757 \n",
      "\n",
      "NEW RECODE!\n",
      "epoch: 56\n",
      "train \t\t loss mean  0.3900 accuracy:  0.9820\n",
      "validataion \t loss mean  0.5245, auc_score :  0.9666, accuracy:  0.9652 \n",
      "\n",
      "epoch: 57\n",
      "train \t\t loss mean  0.3893 accuracy:  0.9826\n",
      "validataion \t loss mean  0.4788, auc_score :  0.9739, accuracy:  0.9743 \n",
      "\n",
      "epoch: 58\n",
      "train \t\t loss mean  0.3901 accuracy:  0.9821\n",
      "validataion \t loss mean  0.4968, auc_score :  0.9705, accuracy:  0.9693 \n",
      "\n",
      "epoch: 59\n",
      "train \t\t loss mean  0.3904 accuracy:  0.9817\n",
      "validataion \t loss mean  0.5018, auc_score :  0.9692, accuracy:  0.9681 \n",
      "\n",
      "epoch: 60\n",
      "train \t\t loss mean  0.3892 accuracy:  0.9830\n",
      "validataion \t loss mean  0.4902, auc_score :  0.9729, accuracy:  0.9722 \n",
      "\n",
      "epoch: 61\n",
      "train \t\t loss mean  0.3893 accuracy:  0.9828\n",
      "validataion \t loss mean  0.5174, auc_score :  0.9676, accuracy:  0.9662 \n",
      "\n",
      "epoch: 62\n",
      "train \t\t loss mean  0.3889 accuracy:  0.9834\n",
      "validataion \t loss mean  0.5361, auc_score :  0.9620, accuracy:  0.9601 \n",
      "\n",
      "epoch: 63\n",
      "train \t\t loss mean  0.3892 accuracy:  0.9834\n",
      "validataion \t loss mean  0.5273, auc_score :  0.9664, accuracy:  0.9652 \n",
      "\n",
      "epoch: 64\n",
      "train \t\t loss mean  0.3882 accuracy:  0.9842\n",
      "validataion \t loss mean  0.4879, auc_score :  0.9736, accuracy:  0.9732 \n",
      "\n",
      "epoch: 65\n",
      "train \t\t loss mean  0.3887 accuracy:  0.9841\n",
      "validataion \t loss mean  0.5205, auc_score :  0.9680, accuracy:  0.9667 \n",
      "\n",
      "epoch: 66\n",
      "train \t\t loss mean  0.3884 accuracy:  0.9840\n",
      "validataion \t loss mean  0.5207, auc_score :  0.9674, accuracy:  0.9667 \n",
      "\n",
      "epoch: 67\n",
      "train \t\t loss mean  0.3882 accuracy:  0.9844\n",
      "validataion \t loss mean  0.5423, auc_score :  0.9629, accuracy:  0.9609 \n",
      "\n",
      "epoch: 68\n",
      "train \t\t loss mean  0.3883 accuracy:  0.9840\n",
      "validataion \t loss mean  0.5091, auc_score :  0.9681, accuracy:  0.9667 \n",
      "\n",
      "epoch: 69\n",
      "train \t\t loss mean  0.3871 accuracy:  0.9852\n",
      "validataion \t loss mean  0.4873, auc_score :  0.9749, accuracy:  0.9745 \n",
      "\n",
      "epoch: 70\n",
      "train \t\t loss mean  0.3876 accuracy:  0.9852\n",
      "validataion \t loss mean  0.5423, auc_score :  0.9658, accuracy:  0.9644 \n",
      "\n",
      "epoch: 71\n",
      "train \t\t loss mean  0.3871 accuracy:  0.9855\n",
      "validataion \t loss mean  0.5468, auc_score :  0.9682, accuracy:  0.9669 \n",
      "\n",
      "epoch: 72\n",
      "train \t\t loss mean  0.3876 accuracy:  0.9849\n",
      "validataion \t loss mean  0.5290, auc_score :  0.9735, accuracy:  0.9730 \n",
      "\n",
      "epoch: 73\n",
      "train \t\t loss mean  0.3874 accuracy:  0.9851\n",
      "validataion \t loss mean  0.5014, auc_score :  0.9707, accuracy:  0.9700 \n",
      "\n",
      "epoch: 74\n",
      "train \t\t loss mean  0.3871 accuracy:  0.9856\n",
      "validataion \t loss mean  0.5365, auc_score :  0.9678, accuracy:  0.9662 \n",
      "\n",
      "epoch: 75\n",
      "train \t\t loss mean  0.3869 accuracy:  0.9856\n",
      "validataion \t loss mean  0.5351, auc_score :  0.9664, accuracy:  0.9648 \n",
      "\n",
      "epoch: 76\n",
      "train \t\t loss mean  0.3867 accuracy:  0.9861\n",
      "validataion \t loss mean  0.5782, auc_score :  0.9614, accuracy:  0.9593 \n",
      "\n",
      "epoch: 77\n",
      "train \t\t loss mean  0.3866 accuracy:  0.9859\n",
      "validataion \t loss mean  0.5093, auc_score :  0.9739, accuracy:  0.9737 \n",
      "\n",
      "epoch: 78\n",
      "train \t\t loss mean  0.3861 accuracy:  0.9864\n",
      "validataion \t loss mean  0.5717, auc_score :  0.9666, accuracy:  0.9652 \n",
      "\n",
      "epoch: 79\n",
      "train \t\t loss mean  0.3866 accuracy:  0.9864\n",
      "validataion \t loss mean  0.6003, auc_score :  0.9639, accuracy:  0.9621 \n",
      "\n",
      "epoch: 80\n",
      "train \t\t loss mean  0.3865 accuracy:  0.9865\n",
      "validataion \t loss mean  0.4862, auc_score :  0.9733, accuracy:  0.9737 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoches = 200\n",
    "\n",
    "best_auc = 0\n",
    "best_epoch = -1\n",
    "best_pred = []\n",
    "\n",
    "prev_model = None\n",
    "\n",
    "for i in range(epoches):\n",
    "    \n",
    "    # Train\n",
    "    loss_sum = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    model.train()\n",
    "    for e_num, (x,y) in enumerate(train_dataloader):\n",
    "        x, y = x.type(torch.FloatTensor).to(device), y.type(torch.FloatTensor).to(device)\n",
    "        model.zero_grad()\n",
    "        pred_y = model(x)\n",
    "        \n",
    "        loss=criterion(pred_y,y)\n",
    "        loss_sum+=loss.detach()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        true_labels.extend(y.cpu().numpy())\n",
    "        pred_labels.extend(np.around(pred_y.cpu().detach().numpy()))\n",
    "        \n",
    "        \n",
    "    acc=accuracy_score(true_labels,pred_labels)\n",
    "    auc = roc_auc_score(true_labels,pred_labels)\n",
    "    print(f'epoch: {i}')\n",
    "    print(f'train \\t loss mean {loss_sum/e_num : .4f}, auc_score : {auc : .4f}, accuracy: {acc : .4f} ')\n",
    "    \n",
    "    # Valid\n",
    "    loss_sum=0\n",
    "    true_labels=[]\n",
    "    pred_labels=[]\n",
    "    model.eval()\n",
    "    for e_num, (x,y) in enumerate(val_dataloader):\n",
    "        x, y = x.type(torch.FloatTensor).to(device), y.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        pred_y = model(x)\n",
    "        loss=criterion(pred_y,y)\n",
    "        \n",
    "        loss_sum+=loss.detach()\n",
    "        \n",
    "        true_labels.extend(y.cpu().numpy())\n",
    "        pred_labels.extend(np.around(pred_y.cpu().detach().numpy()))\n",
    "        \n",
    "        \n",
    "    acc = accuracy_score(true_labels,pred_labels)\n",
    "    auc = roc_auc_score(true_labels,pred_labels)\n",
    "    \n",
    "    print(f'validataion \\t loss mean {loss_sum/e_num : .4f}, auc_score : {auc : .4f}, accuracy: {acc : .4f} ')\n",
    "    if auc > best_auc:\n",
    "        print(\"NEW RECODE!\", end='\\n\\n')\n",
    "        best_pred = pred_labels\n",
    "        best_auc = auc\n",
    "        best_epoch = i\n",
    "        \n",
    "        \n",
    "        if prev_model is not None:\n",
    "            os.remove(prev_model)\n",
    "        prev_model = f'cnn_model_{best_auc : .4f}.h5'\n",
    "        torch.save(model.state_dict(), prev_model)\n",
    "\n",
    "print(f'best validation acc = {best_auc : .4f}, in epoch {best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fper, tper):\n",
    "    plt.plot(fper, tper, color='red', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fper, tper, thresholds = roc_curve(true_labels,best_acc_pred)\n",
    "plot_roc_curve(fper, tper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDAI",
   "language": "python",
   "name": "hdai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
